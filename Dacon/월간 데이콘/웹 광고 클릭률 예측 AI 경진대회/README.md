[월간 Dacon]
<br/>

[목표] 많은 용량의 데이터를 다뤄보자! (index 값이 28,605,390 이였음)
<br/>

[진행과정]
1. data를 train과 test로 나눔
2. data의 결측치 비율, 고유 값의 수를 확인
3. 이를 통해서 high, low cardinality에 대한 기준을 세우고, 결측치 비율에 따라서 feature를 제거함
4. 각각의 train과 test에 대해서 feature들이 어떤 유형인지 (cat, num)인지 확인하고 분류
5. feature importance를 통해 높은 importance와 낮은 importance를 단순 join해보자 <br/>
(하나를 이용했을 때는 낮은 importance를 보이지만 다른 feature와 결합이 되었을 때 새로운 패턴을 보이는 경향이 있었기 때문)
6. scaler를 적용하고 encooding을 진행하고 차원축소를 위해 PCA 진행
7. 이후 CatBoostclassifier를 통해 학습하고 submission 제출
<br/>

[배운 점}
1. 이렇게 많은 양의 데이터는 이전의 공모전과는 차원이 달랐다. (일단 시각화를 통해 데이터를 바라볼 수 없다)
2. 초반에 모델이 돌아가다가 메모리 오류가 매우 많이 발생하였다. (PCA 분석을 통해 차원을 축소시킴으로 해결)
3. 전통적인 ML방식만을 고수했지만, 데이터셋이 매우 큰 경우에는 tabnet등의 DL기반으로 하는 모델도 활용해볼 수 있다는 사실을 알게 되었다.
4. PCA를 진행하는 방식에 대해 공부할 수 있는 계기가 되었다.
5. 늦게나마 딴 사람들의 코드 공유를 통해 알게 된 사실이지만 data type의 변환을 통해 메모리를 줄일 수 있는 사실을 알게 되었다.
<br/>

[한줄평] 이렇게 큰 데이터셋을 ML을 진행해본 것이 처음이랑 많이 미숙했지만 다양한 방법론에 대해 아직 부족하다는 사실을 인지하게 되었고,이 경험이 앞으로 성장하는 밑거름이 되었으면 좋겠다. 
